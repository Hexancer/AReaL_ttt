# ===== Base =====
FROM ubuntu:24.04

WORKDIR /
ENV DEBIAN_FRONTEND=noninteractive

# 基础工具 & Python 工具链
RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates curl wget gnupg git build-essential \
    python3 python3-venv python3-dev python3-pip \
    pkg-config cmake ninja-build patchelf \
    && rm -rf /var/lib/apt/lists/*

# （可选）替换为清华镜像（Ubuntu 24 默认 deb822）
RUN set -eux; \
    if [ -f /etc/apt/sources.list.d/ubuntu.sources ]; then \
      sed -i 's@http://.*archive.ubuntu.com@https://mirrors.tuna.tsinghua.edu.cn@g' /etc/apt/sources.list.d/ubuntu.sources; \
      sed -i 's@http://.*security.ubuntu.com@https://mirrors.tuna.tsinghua.edu.cn@g' /etc/apt/sources.list.d/ubuntu.sources; \
      apt-get update; \
    fi

# ===== CUDA 12.8 toolkit（含 nvcc）=====
# 安装 NVIDIA APT keyring & CUDA 12.8（Ubuntu 24.04）
RUN set -eux; \
    wget -q https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb; \
    dpkg -i cuda-keyring_1.1-1_all.deb; rm -f cuda-keyring_1.1-1_all.deb; \
    apt-get update; \
    apt-get install -y --no-install-recommends cuda-toolkit-12-8; \
    ln -s /usr/local/cuda-12.8 /usr/local/cuda || true; \
    echo "/usr/local/cuda/lib64" > /etc/ld.so.conf.d/cuda.conf; ldconfig; \
    rm -rf /var/lib/apt/lists/*
ENV CUDA_HOME=/usr/local/cuda
ENV PATH="${CUDA_HOME}/bin:${PATH}"
ENV LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"

# ===== Python venv（所有 pip 都走 venv）=====
RUN python3 -m venv /venv
ENV PATH="/venv/bin:${PATH}"

# 升级基础包管理
RUN pip install -U pip wheel setuptools

# ===== 安装 PyTorch 2.7.1（CUDA 12.8）=====
# 官方 cu128 索引。让 pip 从该索引获取 GPU 轮子
ARG TORCH_INDEX=https://download.pytorch.org/whl/cu128
RUN pip install --index-url "${TORCH_INDEX}" \
    "torch==2.7.1" torchvision torchaudio

# ===== 你原有的系统依赖 =====
RUN apt-get update && apt-get install -y --no-install-recommends \
    net-tools kmod ccache \
    libibverbs-dev librdmacm-dev ibverbs-utils \
    rdmacm-utils python3-pyverbs opensm ibutils perftest \
    && rm -rf /var/lib/apt/lists/*

# pip 常用工具
RUN pip install -U uv nvidia-ml-py pipdeptree importlib_metadata packaging platformdirs typing_extensions zipp

# （可选）卸载 RAPIDS / nvfuser 等（如果不存在也不会报错）
RUN pip uninstall -y \
    cugraph-dgl dask-cuda cugraph-service-server raft-dask cugraph cuml \
    cugraph-pyg lightning_thunder opt_einsum nvfuser looseversion lightning_utilities || true

# ===== 编译设置 & 架构 =====
ENV MAX_JOBS=64
ENV TORCH_CUDA_ARCH_LIST="8.0 8.9 9.0 9.0a"

# ===== grouped_gemm =====
RUN pip install "git+https://github.com/fanshiqing/grouped_gemm@v1.1.2"

# ===== cugae =====
RUN git clone https://github.com/garrett4wade/cugae /cugae \
 && pip install -e /cugae --no-build-isolation --verbose

# ===== Megatron-LM =====
RUN git clone -b v0.11.0 https://github.com/NVIDIA/Megatron-LM.git /Megatron-LM \
 && pip install /Megatron-LM \
 && rm -rf /Megatron-LM

# ===== flash-attn v2.6.3 =====
RUN git clone -b v2.6.3 https://github.com/Dao-AILab/flash-attention /flash-attention \
 && cd /flash-attention \
 && git submodule update --init --recursive \
 && pip uninstall -y flash-attn || true \
 && pip install . --no-build-isolation

# ===== flash-attn “3”（hopper 路径 v2.7.2）=====
RUN mkdir -p /flash-attn3 \
 && git clone -b v2.7.2 https://github.com/Dao-AILab/flash-attention /flash-attn3/flash-attention \
 && pip install -v /flash-attn3/flash-attention/hopper/ --no-build-isolation \
 && python - <<'PY'\nimport site,urllib.request,os\nsp=site.getsitepackages()[0]\nd=os.path.join(sp,'flashattn_hopper');os.makedirs(d,exist_ok=True)\nur='https://raw.githubusercontent.com/Dao-AILab/flash-attention/v2.7.2/hopper/flash_attn_interface.py'\nurllib.request.urlretrieve(ur,os.path.join(d,'flash_attn_interface.py'))\nprint('flashattn_hopper interface installed at',d)\nPY \
 && python -c "import torch; print('Torch:', torch.__version__)"

# ===== flashinfer（sglang 依赖）=====
RUN pip install -U setuptools \
 && git clone --recursive -b v0.2.5 https://github.com/flashinfer-ai/flashinfer /flashinfer \
 && FLASHINFER_ENABLE_AOT=1 pip install --no-build-isolation --verbose /flashinfer \
 && rm -rf /flashinfer

# ===== sglang =====
ENV SGL_KERNEL_ENABLE_BF16=1 SGL_KERNEL_ENABLE_FP8=1 SGL_KERNEL_ENABLE_FP4=0
ENV SGL_KERNEL_ENABLE_SM100A=0 SGL_KERNEL_ENABLE_SM90A=1 UV_CONCURRENT_BUILDS=16
RUN git clone -b v0.4.6.post4 https://github.com/sgl-project/sglang.git /sglang \
 && make -C /sglang/sgl-kernel build \
 && pip install /sglang/sgl-kernel/ --force-reinstall --no-build-isolation \
 && pip install -e "/sglang/python[all]" --no-deps

# sglang 依赖
RUN pip install \
    aiohttp requests tqdm numpy IPython setproctitle \
    compressed-tensors datasets decord fastapi hf_transfer huggingface_hub \
    interegular "llguidance>=0.7.11,<0.8.0" modelscope ninja orjson packaging \
    pillow "prometheus-client>=0.20.0" psutil pydantic pynvml python-multipart \
    "pyzmq>=25.1.2" "soundfile==0.13.1" "torchao>=0.7.0" "transformers==4.51.1" \
    uvicorn uvloop "xgrammar==0.1.17" cuda-python "outlines>=0.0.44,<=0.1.11" \
    partial_json_parser einops jsonlines matplotlib pandas sentence_transformers \
    accelerate peft

# ===== vLLM =====
RUN git clone -b v0.8.4 --depth=1 https://github.com/vllm-project/vllm.git /vllm \
 && git config --global http.version HTTP/1.1 \
 && cd /vllm \
 && python3 use_existing_torch.py \
 && pip install -r requirements/build.txt \
 && MAX_JOBS=64 pip install -v . --no-build-isolation \
 && rm -rf /vllm

# ===== AReaL =====
RUN git clone https://code.alipay.com/inclusionAI/AReaL /AReaL \
 && pip install -e /AReaL

# ===== 运行时环境 =====
ENV NVTE_WITH_USERBUFFERS=1 NVTE_FRAMEWORK=pytorch MPI_HOME=/usr/local/mpi
ENV PATH="${PATH}:/opt/hpcx/ompi/bin:/opt/hpcx/ucx/bin"
ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:/opt/hpcx/ompi/lib:/opt/hpcx/ucx/lib/"

WORKDIR /workspace

